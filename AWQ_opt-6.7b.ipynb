{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04591943-5e02-498b-b04a-7d2769490ae1",
   "metadata": {},
   "source": [
    "# Homework\n",
    "## AWQ 实战 - OPT-6.7B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f37952-dad9-432d-bb4c-bb50d1bea8d6",
   "metadata": {},
   "source": [
    "**加载模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc660ae4-740c-4460-89a8-35d78020f177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/LLM/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 9 files: 100%|██████████| 9/9 [00:00<00:00, 35780.79it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/root/miniconda3/envs/LLM/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name_or_path = \"facebook/opt-6.7b\"\n",
    "quant_model_dir = \"models/opt-6.7b-awq\"\n",
    "\n",
    "quant_config = {\n",
    "    \"zero_point\": True,\n",
    "    \"q_group_size\": 128,\n",
    "    \"w_bit\": 4,\n",
    "    \"version\": \"GEMM\"\n",
    "}\n",
    "\n",
    "# 加载模型\n",
    "model = AutoAWQForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055fcae9-5bf5-41c8-babf-0a0228551e4d",
   "metadata": {},
   "source": [
    "**量化**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1ec4319-9f63-4ab9-ae71-21046a54be60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/LLM/lib/python3.11/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n",
      "AWQ: 100%|██████████| 32/32 [09:21<00:00, 17.55s/it]\n"
     ]
    }
   ],
   "source": [
    "# 量化模型\n",
    "model.quantize(tokenizer, quant_config=quant_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9903b6ae-80bc-44cb-a26b-8ce6c427f60c",
   "metadata": {},
   "source": [
    "貌似使用AWQ量化，显存占用要少很多"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7123defa-ba54-4533-ba75-cb0fc858a828",
   "metadata": {},
   "source": [
    "**Transfromers 兼容性配置**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "879f6e98-1fcd-48f7-81e5-3255e21ffb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AwqConfig, AutoConfig\n",
    "\n",
    "# 修改配置文件以使其与transformers集成兼容\n",
    "quantization_config = AwqConfig(\n",
    "    bits=quant_config[\"w_bit\"],\n",
    "    group_size=quant_config[\"q_group_size\"],\n",
    "    zero_point=quant_config[\"zero_point\"],\n",
    "    version=quant_config[\"version\"].lower(),\n",
    ").to_dict()\n",
    "\n",
    "# 预训练的transformers模型存储在model属性中，我们需要传递一个字典\n",
    "model.model.config.quantization_config = quantization_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88b2b60c-dac8-4ad3-a6f4-c419646eb855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/opt-6.7b-awq/tokenizer_config.json',\n",
       " 'models/opt-6.7b-awq/special_tokens_map.json',\n",
       " 'models/opt-6.7b-awq/vocab.json',\n",
       " 'models/opt-6.7b-awq/merges.txt',\n",
       " 'models/opt-6.7b-awq/added_tokens.json',\n",
       " 'models/opt-6.7b-awq/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存模型权重\n",
    "model.save_quantized(quant_model_dir)\n",
    "# 保存分词器\n",
    "tokenizer.save_pretrained(quant_model_dir)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9497bc1e-6fda-4544-9a01-600b3a065fff",
   "metadata": {},
   "source": [
    "### 加载量化模型，并测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac9b9d81-280e-42ab-ad58-75badb8666fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(quant_model_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(quant_model_dir, device_map=\"cuda\").to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc4164ea-ee61-4525-b5ee-231d3b6f4594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "\n",
    "    out = model.generate(**inputs, max_new_tokens=64)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f00b5bb6-e1cf-488c-a6bb-06d9a5a45846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merry Christmas! I'm glad to my family\n",
      " merry merry me! I'm merry! Merry Christmas!\n",
      "Merry Christmas is me and my family!\n"
     ]
    }
   ],
   "source": [
    "result = generate_text(\"Merry Christmas! I'm glad to\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb84a532-768c-464e-91cd-b0da9b1bc77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The woman worked as a caretaker.\n",
      "Yeah a woman who worked as a caretaker.\n",
      "Woman who worked as a caretaker, and they are all a woman who worked as a caretaker? I don't work, you don't work, you don't work.  I didn don work work, you don't work, you\n"
     ]
    }
   ],
   "source": [
    "result = generate_text(\"The woman worked as a\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de84bc7-2a3e-4611-b9cf-3fd487bcc677",
   "metadata": {},
   "source": [
    "**有效果，但是回答一言难尽，不太能用，主要原因还是模型本身**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
